{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/LorgneSchilooch/Quiromanciers/blob/build%2Fmodel/notebook/.ipynb_checkpoints/model_nlp-checkpoint_with_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "bynzt2Mn9fDG",
    "outputId": "6d20d25e-e725-49d3-fb78-7742e9ee8609"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import Sequential, callbacks, utils\n",
    "from tensorflow.python.keras.activations import linear, tanh\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.losses import mse\n",
    "from tensorflow.python.keras.optimizers import SGD, Adam\n",
    "from tensorflow.python import keras\n",
    "# Pad your sequences so they are the same length\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ST4jxE69fDN"
   },
   "outputs": [],
   "source": [
    "# filename = \"../data/biographie_df.csv\"\n",
    "filename = '../data/biographie_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "QV7Uw_m19fDQ",
    "outputId": "83e8663a-5b31-44e8-f9a4-06ed4786b986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            name                                         biographie\n",
      "0     ['Eminem']  Marshall Bruce Mathers III (born October 17, 1...\n",
      "1  ['Lady Gaga']  Stefani Joanne Angelina Germanotta ( STEF-É™n-e...\n"
     ]
    }
   ],
   "source": [
    "df_biographie = pd.read_csv(filename, encoding=\"utf-8\", sep=\";\")\n",
    "print(df_biographie.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "k2Fb7lrq9fDU",
    "outputId": "08177111-6b64-4381-aec2-b2d41631690c"
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u3RcksL49fDY"
   },
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    x = str(x)\n",
    "    x = re.sub(r'\\(.*?\\)', '', x)\n",
    "    x = re.sub(r'\\[.*?\\]', '', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILiZb7SYDW4w"
   },
   "outputs": [],
   "source": [
    "df_biographie['biographie'] = df_biographie['biographie'].apply(lambda x: clean_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pTDG8rzC1X2q"
   },
   "outputs": [],
   "source": [
    "def tag_name(x):\n",
    "    name = x[0]\n",
    "    print(type(name))\n",
    "    names = name.split(' ')\n",
    "    bio = x[1]\n",
    "    if ' , known ' in bio[0:50]:\n",
    "        bio = re.sub('^.* , known ' , '#name , known ', bio)\n",
    "    if ' is ' in bio[0:50]:\n",
    "        bio = re.sub('^.* is ' , '#name is ', bio)\n",
    "    if ' was ' in bio[0:50]:\n",
    "        bio = re.sub('^.* was ' , '#name was ', bio)\n",
    "    bio = bio.replace(name, '#name')\n",
    "    for n in names:\n",
    "        bio = bio.replace(n, '#name')\n",
    "    return bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uTpLdLuv-CkI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "df_biographie['biographie_taged'] = df_biographie.apply(lambda x: tag_name(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "JYxsK2rNDwYC",
    "outputId": "1279a57d-f574-4104-9f2f-9b763f618eb6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>biographie</th>\n",
       "      <th>biographie_taged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['Eminem']</td>\n",
       "      <td>Marshall Bruce Mathers III , known professiona...</td>\n",
       "      <td>#name is also known for his collaborations wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['Lady Gaga']</td>\n",
       "      <td>Stefani Joanne Angelina Germanotta  , known pr...</td>\n",
       "      <td>#name is known for her unconventionality, rein...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                                         biographie  \\\n",
       "0     ['Eminem']  Marshall Bruce Mathers III , known professiona...   \n",
       "1  ['Lady Gaga']  Stefani Joanne Angelina Germanotta  , known pr...   \n",
       "\n",
       "                                    biographie_taged  \n",
       "0  #name is also known for his collaborations wit...  \n",
       "1  #name is known for her unconventionality, rein...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_biographie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wIrr0Pmk9fDe"
   },
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8uFBG_F_e3F"
   },
   "outputs": [],
   "source": [
    "# import en_core_web_lg\n",
    "# import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fQqbouqD_hg-"
   },
   "outputs": [],
   "source": [
    "# nlp_en = en_core_web_lg.load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cxUZt5ni9fDh",
    "outputId": "8f63a5a7-7e0a-42eb-b011-75a35f21f1e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function en_core_web_lg.load>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "eub0TksJ9fDl",
    "outputId": "3335d7c7-18be-48e5-88b8-bb98b03a9f04"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f04c3222ccc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m for doc in nlp_en.pipe(df_biographie['biographie'].astype('unicode').values, batch_size=50,\n\u001b[0m\u001b[1;32m      6\u001b[0m                         n_threads=3):\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'pipe'"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "for doc in nlp_en.pipe(df_biographie['biographie'].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "df_biographie['species_tokens'] = tokens\n",
    "df_biographie['species_lemma'] = lemma\n",
    "df_biographie['species_pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ApqB7Ic9fDo",
    "outputId": "ce633b0a-9686-4664-83dd-41e0b549130a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text = '\\n'.join([''.join(sentence) for sentence in df_biographie['species_tokens'][0]])\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaySy4pT9fDs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import Sequential, callbacks, utils\n",
    "from tensorflow.python.keras.activations import linear, tanh\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.losses import mse\n",
    "from tensorflow.python.keras.optimizers import SGD, Adam\n",
    "from tensorflow.python import keras\n",
    "class charLSTMmodel():\n",
    "    \n",
    "    def fit(self,text,epochs=100):\n",
    "        self._load(text)\n",
    "        self._build()\n",
    "        self._train(epochs)\n",
    "        \n",
    "    def _load(self, text):\n",
    "        self.idx_token = dict(enumerate(set(self._tokenise(text)),start=2))\n",
    "        self.idx_token[0] = '<PAD>'\n",
    "        self.idx_token[1] = '<UNK>' \n",
    "        self.token_idx = {word:i for i,word in self.idx_token.items()}       \n",
    "        token_ids = [[self.token_idx[token] for token in self._tokenise(sentence)] for sentence in self._chunk(text)]\n",
    "        inouts = [tokens[:i+1] for tokens in token_ids for i in range(1,len(tokens))]\n",
    "        self.x_dim = max([len(x) for x in inouts]) - 1\n",
    "        self.y_dim = len(self.idx_token) \n",
    "        inouts = np.array(keras.preprocessing.sequence.pad_sequences(inouts,maxlen=self.x_dim + 1, padding='pre'))\n",
    "        self.X, self.Y = inouts[:,:-1], inouts[:,-1]\n",
    "        \n",
    "    def _tokenise(self,text):\n",
    "        return list(' '.join(text.split()).replace(\" \",\"_\"))\n",
    "\n",
    "    def generate(self,words,i=150):\n",
    "        for _ in range(i):\n",
    "            x = [self.token_idx[token] if token in self.token_idx else 1 for token in self._tokenise(words)] \n",
    "            x = keras.preprocessing.sequence.pad_sequences([x], maxlen=self.x_dim, padding = 'pre')\n",
    "            y_hat = self.model.predict_classes(x, verbose=0)[0] #maximise\n",
    "            words += self.idx_token[y_hat]\n",
    "            return words.replace(\"_\",\" \")\n",
    "    \n",
    "    def _chunk(self,text,chunk_size = 100):\n",
    "        return ''.join([c + '<S>' if not i % chunk_size else c for i,c in enumerate(text,start=1)]).split('<S>')\n",
    "\n",
    "    def _build(self):\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.Embedding(self.y_dim, 10, input_length=self.x_dim))\n",
    "        self.model.add(keras.layers.LSTM(150, return_sequences = True))\n",
    "        self.model.add(keras.layers.LSTM(100))\n",
    "        self.model.add(keras.layers.Dense(self.y_dim, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def _train(self,epochs):\n",
    "        earlystop =  keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "        onehot_y = keras.utils.to_categorical(self.Y, num_classes=self.y_dim)\n",
    "        self.model.fit(self.X, onehot_y, epochs=epochs, verbose=1, callbacks=[earlystop])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vDa6yzDY9fDv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-vGByMdC9fDz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1CgDlKe-9fD2"
   },
   "outputs": [],
   "source": [
    "clstm = charLSTMmodel()\n",
    "clstm.fit(text, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DvawZ7Yw9fD6"
   },
   "outputs": [],
   "source": [
    "clstm.generate(\"my name is Manitra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aq3hRsc19fD9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "model_nlp-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
