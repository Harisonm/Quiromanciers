{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mranaivoharison/opt/anaconda3/envs/py37nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import Sequential, callbacks, utils\n",
    "from tensorflow.python.keras.activations import linear, tanh\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.losses import mse\n",
    "from tensorflow.python.keras.optimizers import SGD, Adam\n",
    "from tensorflow.python import keras\n",
    "# Pad your sequences so they are the same length\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/biographie_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            name                                         biographie\n",
      "0         Eminem  Marshall Bruce Mathers III (born October 17, 1...\n",
      "1      Lady Gaga  Stefani Joanne Angelina Germanotta ( STEF-ən-e...\n",
      "2  Justin Bieber  Justin Drew Bieber (; born March 1, 1994) is a...\n",
      "3      Lil Wayne  Dwayne Michael Carter Jr. (born September 27, ...\n",
      "4    Miley Cyrus  Miley Ray Hemsworth (née Cyrus, born Destiny H...\n"
     ]
    }
   ],
   "source": [
    "df_biographie = pd.read_csv(filename, encoding=\"utf-8\", sep=\";\")\n",
    "print(df_biographie.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x13ec7ae10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "for doc in nlp_en.pipe(df_biographie['biographie'].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "df_biographie['species_tokens'] = tokens\n",
    "df_biographie['species_lemma'] = lemma\n",
    "df_biographie['species_pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "text = '\\n'.join([''.join(sentence) for sentence in df_biographie['species_tokens'][0]])\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import Sequential, callbacks, utils\n",
    "from tensorflow.python.keras.activations import linear, tanh\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.losses import mse\n",
    "from tensorflow.python.keras.optimizers import SGD, Adam\n",
    "from tensorflow.python import keras\n",
    "class charLSTMmodel():\n",
    "    \n",
    "    def fit(self,text,epochs=100):\n",
    "        self._load(text)\n",
    "        self._build()\n",
    "        self._train(epochs)\n",
    "        \n",
    "    def _load(self, text):\n",
    "        self.idx_token = dict(enumerate(set(self._tokenise(text)),start=2))\n",
    "        self.idx_token[0] = '<PAD>'\n",
    "        self.idx_token[1] = '<UNK>' \n",
    "        self.token_idx = {word:i for i,word in self.idx_token.items()}       \n",
    "        token_ids = [[self.token_idx[token] for token in self._tokenise(sentence)] for sentence in self._chunk(text)]\n",
    "        inouts = [tokens[:i+1] for tokens in token_ids for i in range(1,len(tokens))]\n",
    "        self.x_dim = max([len(x) for x in inouts]) - 1\n",
    "        self.y_dim = len(self.idx_token) \n",
    "        inouts = np.array(keras.preprocessing.sequence.pad_sequences(inouts,maxlen=self.x_dim + 1, padding='pre'))\n",
    "        self.X, self.Y = inouts[:,:-1], inouts[:,-1]\n",
    "        \n",
    "    def _tokenise(self,text):\n",
    "        return list(' '.join(text.split()).replace(\" \",\"_\"))\n",
    "\n",
    "    def generate(self,words,i=150):\n",
    "        for _ in range(i):\n",
    "            x = [self.token_idx[token] if token in self.token_idx else 1 for token in self._tokenise(words)] \n",
    "            x = keras.preprocessing.sequence.pad_sequences([x], maxlen=self.x_dim, padding = 'pre')\n",
    "            y_hat = self.model.predict_classes(x, verbose=0)[0] #maximise\n",
    "            words += self.idx_token[y_hat]\n",
    "            return words.replace(\"_\",\" \")\n",
    "    \n",
    "    def _chunk(self,text,chunk_size = 100):\n",
    "        return ''.join([c + '<S>' if not i % chunk_size else c for i,c in enumerate(text,start=1)]).split('<S>')\n",
    "\n",
    "    def _build(self):\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.Embedding(self.y_dim, 10, input_length=self.x_dim))\n",
    "        self.model.add(keras.layers.LSTM(150, return_sequences = True))\n",
    "        self.model.add(keras.layers.LSTM(100))\n",
    "        self.model.add(keras.layers.Dense(self.y_dim, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def _train(self,epochs):\n",
    "        earlystop =  keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "        onehot_y = keras.utils.to_categorical(self.Y, num_classes=self.y_dim)\n",
    "        self.model.fit(self.X, onehot_y, epochs=epochs, verbose=1, callbacks=[earlystop])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstm = charLSTMmodel()\n",
    "clstm.fit(text, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstm.generate(\"my name is Manitra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
