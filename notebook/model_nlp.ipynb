{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import Sequential, callbacks, utils\n",
    "from tensorflow.python.keras.activations import linear, tanh\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.losses import mse\n",
    "from tensorflow.python.keras.optimizers import SGD, Adam\n",
    "from tensorflow.python import keras\n",
    "# Pad your sequences so they are the same length\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/biographie_df.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biographie = pd.read_csv(filename, encoding=\"utf-8\", sep=\";\")\n",
    "print(df_biographie.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "for doc in nlp_en.pipe(df_biographie['biographie'].astype('unicode').values, batch_size=50,\n",
    "                        n_threads=3):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "df_biographie['species_tokens'] = tokens\n",
    "df_biographie['species_lemma'] = lemma\n",
    "df_biographie['species_pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biographie['species_lemma'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n'.join([''.join(sentence) for sentence in df_biographie['species_tokens'][0]])\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((df_biographie.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = df_biographie.cumulative_input_vectors.apply(len).max()\n",
    "# Save it as a list   \n",
    "padded_sequences = pad_sequences(df_biographie.cumulative_input_vectors.tolist(), max_sequence_length).tolist()\n",
    "df_biographie['padded_input_vectors'] = pd.Series(padded_sequences).apply(np.asarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import Sequential, callbacks, utils\n",
    "from tensorflow.python.keras.activations import linear, tanh\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.losses import mse\n",
    "from tensorflow.python.keras.optimizers import SGD, Adam\n",
    "from tensorflow.python import keras\n",
    "class charLSTMmodel():\n",
    "    \n",
    "    def fit(self,text,epochs=100):\n",
    "        self._load(text)\n",
    "        self._build()\n",
    "        self._train(epochs)\n",
    "        \n",
    "    def _load(self, text):\n",
    "        self.idx_token = dict(enumerate(set(self._tokenise(text)),start=2))\n",
    "        self.idx_token[0] = '<PAD>'\n",
    "        self.idx_token[1] = '<UNK>' \n",
    "        self.token_idx = {word:i for i,word in self.idx_token.items()}       \n",
    "        token_ids = [[self.token_idx[token] for token in self._tokenise(sentence)] for sentence in self._chunk(text)]\n",
    "        inouts = [tokens[:i+1] for tokens in token_ids for i in range(1,len(tokens))]\n",
    "        self.x_dim = max([len(x) for x in inouts]) - 1\n",
    "        self.y_dim = len(self.idx_token) \n",
    "        inouts = np.array(keras.preprocessing.sequence.pad_sequences(inouts,maxlen=self.x_dim + 1, padding='pre'))\n",
    "        self.X, self.Y = inouts[:,:-1], inouts[:,-1]\n",
    "        \n",
    "    def _tokenise(self,text):\n",
    "        return list(' '.join(text.split()).replace(\" \",\"_\"))\n",
    "\n",
    "    def generate(self,words,i=150):\n",
    "        for _ in range(i):\n",
    "            x = [self.token_idx[token] if token in self.token_idx else 1 for token in self._tokenise(words)] \n",
    "            x = keras.preprocessing.sequence.pad_sequences([x], maxlen=self.x_dim, padding = 'pre')\n",
    "            y_hat = self.model.predict_classes(x, verbose=0)[0] #maximise\n",
    "            words += self.idx_token[y_hat]\n",
    "            return words.replace(\"_\",\" \")\n",
    "    \n",
    "    def _chunk(self,text,chunk_size = 100):\n",
    "        return ''.join([c + '<S>' if not i % chunk_size else c for i,c in enumerate(text,start=1)]).split('<S>')\n",
    "\n",
    "    def _build(self):\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.Embedding(self.y_dim, 10, input_length=self.x_dim))\n",
    "        self.model.add(keras.layers.LSTM(150, return_sequences = True))\n",
    "        self.model.add(keras.layers.LSTM(100))\n",
    "        self.model.add(keras.layers.Dense(self.y_dim, activation='softmax'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    def _train(self,epochs):\n",
    "        earlystop =  keras.callbacks.EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "        onehot_y = keras.utils.to_categorical(self.Y, num_classes=self.y_dim)\n",
    "        self.model.fit(self.X, onehot_y, epochs=epochs, verbose=1, callbacks=[earlystop])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstm = charLSTMmodel()\n",
    "clstm.fit(text, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstm.generate(\"my name is Manitra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
